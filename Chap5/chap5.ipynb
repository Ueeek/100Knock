{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CaboCha\n",
    "import re\n",
    "import pydot_ng as pydot\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse():\n",
    "    '''\n",
    "    neko.txt.cabpchaを作る\n",
    "    '''\n",
    "    with open(\"./neko.txt\") as f:\n",
    "        with open(\"./neko.txt.cabocha\",\"w\") as outFile:\n",
    "            cabocha = CaboCha.Parser()\n",
    "            for line in f:\n",
    "                outFile.write(cabocha.parse(line).toString(CaboCha.FORMAT_LATTICE))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Morph:\n",
    "    '''\n",
    "    形態素を表すクラス\n",
    "    surface:表層系\n",
    "    base:基本形\n",
    "    pos:品詞\n",
    "    po1:品詞再分類\n",
    "    '''\n",
    "    def __init__(self,surface,base,pos,pos1):\n",
    "        self.surface=surface\n",
    "        self.base=base\n",
    "        self.pos=pos\n",
    "        self.pos1=pos1\n",
    "    \n",
    "    def __str__(self):\n",
    "        '''オブジェクトの文字列表現'''\n",
    "        return 'surface[{}]\\tbase[{}]\\tpos[{}]\\tpos1[{}]'\\\n",
    "            .format(self.surface, self.base, self.pos, self.pos1)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neco_lines():\n",
    "    '''\n",
    "    係り受け解析結果のジェネレータ\n",
    "    '''\n",
    "    with open(\"./neko.txt.cabocha\") as f:\n",
    "        morphs=[]\n",
    "        for line in f:\n",
    "            if line==\"EOS\\n\":\n",
    "                yield morphs\n",
    "                morphs=[]\n",
    "            else:\n",
    "                if line[0]=='*':\n",
    "                    continue\n",
    "                else:\n",
    "                    cols = line.split('\\t')\n",
    "                    res_cols=cols[1].split(',')\n",
    "                    newM=Morph(cols[0],res_cols[6],res_cols[0],res_cols[1])\n",
    "                    print(\"NewEntry\")\n",
    "                    print(newM)\n",
    "                    morphs.append(\n",
    "                       newM\n",
    "                    )\n",
    "    raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,morphs in enumerate(neco_lines(),1):\n",
    "    print(\"i->\",i)\n",
    "    if i==10:\n",
    "        for morph in morphs:\n",
    "            print(morph)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chunk:\n",
    "    '''\n",
    "    文節を表す\n",
    "    morphs:Morphのkisut\n",
    "    dst: かかり先文節のインデックス番号\n",
    "    srcs:かかり元文節のインデックス番号のlist\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.morphs=[]\n",
    "        self.srcs=[]\n",
    "        self.dst=-1\n",
    "    \n",
    "    def __str__(self):\n",
    "        \n",
    "        surface = ''\n",
    "        for morph in self.morphs:\n",
    "            surface += morph.surface\n",
    "        return '{}\\tsrcs{}\\tdst[{}]'.format(surface, self.srcs, self.dst)\n",
    "    \n",
    "    def normalized_surface(self):\n",
    "        result=''\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos != '記号':\n",
    "                result += morph.surface\n",
    "        return result\n",
    "    \n",
    "    def contain_pos(self,searchPos):\n",
    "        '''\n",
    "        chunkのなかにseachPos(品詞名)を含むか\n",
    "        ret:bool\n",
    "        '''        \n",
    "        for morph in self.morphs:\n",
    "            if morph.pos==searchPos:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def get_pos_in_morphs(self,pos,pos1=\"\"):\n",
    "        ret=[]\n",
    "        if len(pos1)>0:\n",
    "            return [res for res in self.morphs if res.pos==pos and res.pos1==pos1]\n",
    "        else:\n",
    "            return [res for res in self.morphs if res.pos==pos]\n",
    "        \n",
    "    def get_kaku_prt(self):\n",
    "        prts = self.get_pos_in_morphs('助詞')\n",
    "        if len(prts)>1:\n",
    "            kaku_prts=self.get_pos_in_morphs('助詞','格助詞')\n",
    "            if len(kaku_prts)>0:\n",
    "                prts = kaku_prts\n",
    "        if len(prts)>0:\n",
    "            return prts[-1].surface\n",
    "        else:\n",
    "            return ''\n",
    "        \n",
    "    def get_sahen_wo(self):\n",
    "        '''\n",
    "        [さ変接続名詞+を]を含む場合は、surfaceを返す\n",
    "        '''\n",
    "        for i,morph in enumerate(self.morphs[0:-1]):\n",
    "            if morph.pos==\"名詞\" \\\n",
    "                and morph.pos1==\"サ変接続\" \\\n",
    "                and self.morphs[i+1].pos ==\"助詞\" \\\n",
    "                and self.morphs[i+1].surface==\"を\":\n",
    "                    #複数ある場合はもっとも左の動詞のみでいいから、return してる\n",
    "                    return morph.surface+self.morphs[i+1].surface\n",
    "        return ''\n",
    "                                             \n",
    "    \n",
    "    def noun_masked_surface(self,mask,dst=False):\n",
    "        '''\n",
    "        名詞を指定文字(mask)でマスクしたsurfaceを返す\n",
    "        '''\n",
    "        result =''\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos != '記号':\n",
    "                if morph.pos==\"名詞\":\n",
    "                    result+=mask\n",
    "                    if dst:\n",
    "                        return result\n",
    "                masK=''\n",
    "            else:\n",
    "                result += morph.surface\n",
    "        return result\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_lines():\n",
    "    with open(\"./neko.txt.cabocha\") as file:\n",
    "        chunks=dict()\n",
    "        idx=-1\n",
    "        \n",
    "        for line in file:\n",
    "            if line==\"EOS\\n\":\n",
    "                if len(chunks)>0:\n",
    "                    sorted_tuple = sorted(chunks.items(),key=lambda x:x[0])\n",
    "                    yield list(zip(*sorted_tuple))[1]\n",
    "                    chunks.clear()\n",
    "                else:\n",
    "                    yield []\n",
    "                \n",
    "            elif line[0]==\"*\":\n",
    "                cols=line.split(' ')\n",
    "                idx=int(cols[1])\n",
    "                dst = int(re.search(r'(.*?)D',cols[2]).group(1))\n",
    "                \n",
    "                if idx not in chunks:\n",
    "                    chunks[idx]=Chunk()\n",
    "                chunks[idx].dst=dst\n",
    "                \n",
    "                if dst != -1:\n",
    "                    if dst not in chunks:\n",
    "                        chunks[dst]=Chunk()\n",
    "                    chunks[dst].srcs.append(idx)\n",
    "            else:\n",
    "                cols = line.split(\"\\t\")\n",
    "                res_cols=cols[1].split(\",\")\n",
    "                \n",
    "                chunks[idx].morphs.append(\n",
    "                    Morph(\n",
    "                        cols[0],\n",
    "                        res_cols[6],\n",
    "                        res_cols[0],\n",
    "                        res_cols[1])\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,chunks in enumerate(chunk_lines(),1):\n",
    "    if i==8:\n",
    "        for j,chunk in enumerate(chunks):\n",
    "            print(\"[{}]{}\".format(j,chunk))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunks in chunk_lines():\n",
    "    for chunk in chunks:\n",
    "        if chunk.dst != -1:\n",
    "            src = chunk.normalized_surface()\n",
    "            dst = chunks[chunk.dst].normalized_surface()\n",
    "            if src != '' and dst != '':\n",
    "                print(\"{}\\t{}\".format(src,dst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunks in chunk_lines():\n",
    "    for chunk in chunks:\n",
    "        if chunk.dst != -1:\n",
    "            src = chunk\n",
    "            dst = chunks[chunk.dst]\n",
    "            if src.contain_pos(\"名詞\") and dst.contain_pos(\"動詞\"):\n",
    "                src_str=src.normalized_surface()\n",
    "                dst_str= dst.normalized_surface()\n",
    "                if src_str:\n",
    "                    print(src_str+\"\\t\"+dst_str)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createGraph(edges,directed=False):\n",
    "    '''\n",
    "    与えられた分の係り受け木を生成する\n",
    "    '''\n",
    "    if directed:\n",
    "        graph = pydot.Dot(graph_type=\"digraph\")\n",
    "    else:\n",
    "        graph = pydot.Dot(graph_type=\"graph\")\n",
    "        \n",
    "    for a,b in edges:\n",
    "        id1 = str(a[0])\n",
    "        id2 = str(b[0])\n",
    "        label1 = str(a[1])\n",
    "        label2 = str(b[1])\n",
    "        \n",
    "        graph.add_node(pydot.Node(id1,label=label1))\n",
    "        graph.add_node(pydot.Node(id2,label=label2))\n",
    "        \n",
    "        graph.add_edge(pydot.Edge(id1,id2))\n",
    "        \n",
    "    return graph\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges=[]\n",
    "def createEdges():\n",
    "    for chunks in chunk_lines():\n",
    "        edges=[]\n",
    "        for i,chunk in enumerate(chunks):\n",
    "            if chunk.dst != -1:\n",
    "                src = chunk.normalized_surface()\n",
    "                dst = chunks[chunk.dst].normalized_surface()\n",
    "                if src!='' and dst!= '':\n",
    "                    edges.append([[i,src],[chunk.dst,dst]])\n",
    "        if len(edges)>8:\n",
    "            return edges\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges=createEdges()\n",
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(edges)>0:\n",
    "    graph = createGraph(edges,directed=True)\n",
    "    graph.write_png(\"./result.png\")\n",
    "else:\n",
    "    print(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"res_45.txt\",\"w\") as outFile:\n",
    "    for chunks in chunk_lines():\n",
    "        for chunk in chunks:\n",
    "            verbs = chunk.get_pos_in_morphs(\"動詞\")\n",
    "            if len(verbs)==0:\n",
    "                #動詞を含まない\n",
    "                continue\n",
    "            #かかり元の列挙\n",
    "            prts=[]\n",
    "            for src in chunk.srcs:\n",
    "                prts_in_chunk = chunks[src].get_pos_in_morphs(\"助詞\")\n",
    "                if len(prts_in_chunk)>1:\n",
    "                    kaku_prts = chunks[src].get_pos_in_morphs(\"助詞\",\"各助詞\")\n",
    "                    if len(kaku_prts)>0:\n",
    "                        prts_in_chunk=kaku_prts\n",
    "                if len(prts_in_chunk)>0:\n",
    "                    prts.append(prts_in_chunk[-1])\n",
    "            if len(prts)<1:\n",
    "                continue\n",
    "            outFile.write(\"{}\\t{}\\n\".format(verbs[0].base,' '.join(sorted(prt.surface for prt in prts))))\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head res_45.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unix　確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sort res_45.txt | uniq -c | sort --numeric-sort --reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! grep \"^する\\s\" res_45.txt | sort | uniq -c | sort --numeric-sort --reverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"res_46.txt\",\"w\") as outFile:\n",
    "    for chunks in chunk_lines():\n",
    "        for chunk in chunks:\n",
    "            verbs = chunk.get_pos_in_morphs(\"動詞\")\n",
    "            if len(verbs)==0:\n",
    "                #動詞を含まない\n",
    "                continue\n",
    "            #かかり元に助詞を含むchunkを列挙\n",
    "            chunks_include_prt=[]\n",
    "            for src in chunk.srcs:\n",
    "                chunks_include_prt.append(chunks[src])\n",
    "            if len(chunk_include_prt)<1:\n",
    "                continue\n",
    "            chunks_include_prt = sorted(chunks_include_prt,key=lambda x:x.get_kaku_prt())\n",
    "            \n",
    "            outFile.write('{}\\t{}\\t{}\\n'.format(\n",
    "                verbs[0].base,\n",
    "                ' '.join([chunk.get_kaku_prt() for chunk in chunks_include_prt]),\n",
    "                ' '.join([chunk.normalized_surface() for chunk in chunks_include_prt])\n",
    "            ))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head res_46.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"res_47.txt\",\"w\") as out_file:\n",
    "    for chunks in chunk_lines():\n",
    "        for chunk in chunks:\n",
    "            verbs = chunk.get_pos_in_morphs(\"動詞\")\n",
    "            if len(verbs)<1:\n",
    "                continue\n",
    "            \n",
    "            chunks_include_prt=[]\n",
    "            for src in chunk.srcs:\n",
    "                if len(chunks[src].get_kaku_prt()) >0:\n",
    "                    chunks_include_prt.append(chunks[src])\n",
    "            if len(chunks_include_prt)<1:\n",
    "                continue\n",
    "            \n",
    "            sahen_wo=''\n",
    "            for chunk_src in chunks_include_prt:\n",
    "                sahen_wo = chunk_src.get_sahen_wo()\n",
    "                if len(sahen_wo)>0:\n",
    "                    chunk_remove = chunk_src\n",
    "                    break\n",
    "            if len(sahen_wo)<1:\n",
    "                continue\n",
    "                \n",
    "            chunks_include_prt.remove(chunk_remove)\n",
    "            \n",
    "            chunks_include_prt = sorted(chunks_include_prt,key=lambda x:x.get_kaku_prt())\n",
    "            \n",
    "            out_file.write(\"{}\\t{}\\t{}\\n\".format(\n",
    "                           sahen_wo+verbs[0].base,\n",
    "                            ' '.join([chunk.get_kaku_prt() for chunk in chunks_include_prt]),\n",
    "                            ' '.join([chunk.normalized_surface() for chunk in chunks_include_prt])\n",
    "            ))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head res_47.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"res_48.txt\",\"w\") as out_file:\n",
    "    for chunks in chunk_lines():\n",
    "        for chunk in chunks:\n",
    "            if len(chunk.get_pos_in_morphs('名詞'))>0:\n",
    "                out_file.write(chunk.normalized_surface())\n",
    "                dst = chunk.dst\n",
    "                while dst != -1:\n",
    "                    out_file.write(' ->' + chunks[dst].normalized_surface())\n",
    "                    dst = chunks[dst].dst\n",
    "                out_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head res_48.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"res_49.txt\",\"w\") as out_file:\n",
    "    for chunks in chunk_lines():\n",
    "        indexs_noun = [i for i in range(len(chunks)) if len(chunks[i].get_pos_in_morphs('名詞'))>0]\n",
    "        \n",
    "        \n",
    "        if len(indexs_noun)<2:\n",
    "            continue\n",
    "        \n",
    "        for i,index_x in enumerate(indexs_noun[:-1]):\n",
    "            for index_y in indexs_noun[i+1:]:\n",
    "                meet_y=False\n",
    "                index_dup=-1\n",
    "                routes_x=set()\n",
    "                \n",
    "                dst = chunks[index_x].dst\n",
    "                while dst!=-1:\n",
    "                    if dst==index_y:\n",
    "                        meet_y=True\n",
    "                        break\n",
    "                    routes_x.add(dst)\n",
    "                    dst = chunks[dst].dst\n",
    "                \n",
    "                if not meet_y:\n",
    "                    dst = chunks[index_y].dst\n",
    "                    while dst != -1:\n",
    "                        if dst in routes_x:\n",
    "                            index_dup = dst\n",
    "                            break\n",
    "                        else:\n",
    "                            dst = chunks[dst].dst\n",
    "                if index_dup==-1:\n",
    "                    out_file.write(chunks[index_x].noun_masked_surface(\"X\"))\n",
    "                    dst = chunks[index_x].dst\n",
    "                    while dst != -1:\n",
    "                        if dst == index_y:\n",
    "                            out_file.write(\" ->\" + chunks[dst].noun_masked_surface('Y',True))\n",
    "                            break\n",
    "                        else:\n",
    "                            out_file.write(\" ->\" +chunks[dst].normalized_surface())\n",
    "                        dst = chunks[dst].dst\n",
    "                    out_file.write(\"\\n\")\n",
    "                    \n",
    "                else:\n",
    "                    out_file.write(chunks[index_x].noun_masked_surface('X'))\n",
    "                    dst = chunks[index_x].dst\n",
    "                    while dst != index_dup:\n",
    "                        out_file.write(\" ->\" + chunks[dst].normalized_surface())\n",
    "                        dst = chunks[dst].dst\n",
    "                    out_file.write(\" | \")\n",
    "                    \n",
    "                    out_file.write(chunks[index_y].noun_masked_surface('Y'))\n",
    "                    dst = chunks[index_y].dst\n",
    "                    while dst != index_dup:\n",
    "                        out_file.write(\" -> \"+chunks[dst].normalized_surface())\n",
    "                        dst = chunks[dst].dst\n",
    "                    out_file.write(\" | \")\n",
    "                    \n",
    "                    out_file.write(chunks[index_dup].normalized_surface())\n",
    "                    out_file.write(\"\\n\")\n",
    "                    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head res_49.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
