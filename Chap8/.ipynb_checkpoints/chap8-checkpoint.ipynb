{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import random\n",
    "from nltk.stem.porter import PorterStemmer as PS\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_pos = 'rt-polaritydata/rt-polaritydata/rt-polarity.pos'\n",
    "fname_neg = 'rt-polaritydata/rt-polaritydata/rt-polarity.neg'\n",
    "fname_smt = 'sentiment.txt'\n",
    "fencoding='cp1252'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(fname_pos,'r',fencoding) as file_pos:\n",
    "    result.extend(['+1 {}'.format(line.strip()) for line in file_pos])\n",
    "\n",
    "with codecs.open(fname_neg,'r', fencoding) as file_neg:\n",
    "    result.extend(['-1 {}'.format(line.strip()) for line in file_neg])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(fname_smt,'w',fencoding) as file_out:\n",
    "    print(*result,sep='\\n',file=file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_pos=0\n",
    "cnt_neg=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos:5331, neg:5331\n"
     ]
    }
   ],
   "source": [
    "with codecs.open(fname_smt,'r',fencoding) as file_out:\n",
    "    for line in file_out:\n",
    "        if line.startswith('+1'):\n",
    "            cnt_pos+=1\n",
    "        elif line.startswith('-1'):\n",
    "            cnt_neg+=1\n",
    "print(\"pos:{}, neg:{}\".format(cnt_pos,cnt_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = (\n",
    "    'a,able,about,across,after,all,almost,also,am,among,an,and,any,are,'\n",
    "    'as,at,be,because,been,but,by,can,cannot,could,dear,did,do,does,'\n",
    "    'either,else,ever,every,for,from,get,got,had,has,have,he,her,hers,'\n",
    "    'him,his,how,however,i,if,in,into,is,it,its,just,least,let,like,'\n",
    "    'likely,may,me,might,most,must,my,neither,no,nor,not,of,off,often,'\n",
    "    'on,only,or,other,our,own,rather,said,say,says,she,should,since,so,'\n",
    "    'some,than,that,the,their,them,then,there,these,they,this,tis,to,too,'\n",
    "    'twas,us,wants,was,we,were,what,when,where,which,while,who,whom,why,'\n",
    "    'will,with,would,yet,you,your').lower().split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_stopword(str):\n",
    "    '''\n",
    "    文字がストップワードかどうかをboolで返す\n",
    "    '''\n",
    "    return str.lower() in stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正しく検出されることのテスト\n",
    "assert is_stopword('a')             # リストの先頭\n",
    "assert is_stopword('your')          # リストの末尾\n",
    "assert is_stopword('often')         # リストの中間\n",
    "assert is_stopword('on')            # リストの中間\n",
    "assert is_stopword('A')             # 大小文字の同一視\n",
    "assert is_stopword('Your')          # 大小文字の同一視\n",
    "assert is_stopword('ofteN')         # 大小文字の同一視\n",
    "assert is_stopword('ON')            # 大小文字の同一視\n",
    "\n",
    "# 誤検出されないことのテスト\n",
    "assert not is_stopword('0')         # リストにない\n",
    "assert not is_stopword('z')         # リストにない\n",
    "assert not is_stopword('bout')      # 後方一致されない\n",
    "assert not is_stopword('acros')     # 前方一致されない\n",
    "assert not is_stopword('fte')       # 中間一致されない\n",
    "assert not is_stopword(' ')         # 空白\n",
    "assert not is_stopword('\\n')        # 制御コード\n",
    "assert not is_stopword('')          # 空文字"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_sentiment = 'sentiment.txt'\n",
    "fname_features = 'features.txt'\n",
    "fname_theta='theta.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#素性抽出\n",
    "ps=PS()\n",
    "word_counter = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(fname_sentiment,'r',fencoding) as file_in:\n",
    "    for line in file_in:\n",
    "        for word in line[3:].split(' '):\n",
    "            word = word.strip()\n",
    "            \n",
    "            if is_stopword(word):\n",
    "                continue\n",
    "            else:\n",
    "                word = ps.stem(word)\n",
    "                \n",
    "                if word != '!' and word != '?' and len(word) <=1:\n",
    "                    continue\n",
    "                word_counter.update([word])\n",
    "features = [word for word,count in word_counter.items() if count >= 6]\n",
    "    \n",
    "with codecs.open(fname_features,'w',fencoding) as file_out:\n",
    "    print(*features,sep='\\n',file=file_out)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dumb\r\n",
      "exploit\r\n",
      "violenc\r\n",
      "iron\r\n",
      "becom\r\n",
      "everyth\r\n",
      "clumsi\r\n",
      "origin\r\n",
      "against\r\n",
      "exercis\r\n",
      "chill\r\n",
      "style\r\n",
      "film\r\n",
      "insid\r\n",
      "out\r\n",
      "eye\r\n",
      "sens\r\n",
      "mysteri\r\n",
      "frailti\r\n",
      "offer\r\n",
      "much\r\n",
      "those\r\n",
      "sit\r\n",
      "around\r\n",
      "midnight\r\n",
      "tell\r\n",
      "creepi\r\n",
      "stori\r\n",
      "give\r\n",
      "each\r\n",
      "willi\r\n",
      "there'\r\n",
      "way\r\n",
      "won't\r\n",
      "talk\r\n",
      "onc\r\n",
      "theater\r\n",
      "parker\r\n",
      "display\r\n",
      "play\r\n",
      "class\r\n",
      "wild\r\n",
      "himself\r\n",
      "littl\r\n",
      "question\r\n",
      "seriou\r\n",
      "work\r\n",
      "import\r\n",
      "director\r\n",
      "someth\r\n",
      "new\r\n",
      "reel\r\n",
      "love\r\n",
      "david\r\n",
      "one\r\n",
      "told\r\n",
      "entir\r\n",
      "point\r\n",
      "view\r\n",
      "movi\r\n",
      "such\r\n",
      "excel\r\n",
      "job\r\n",
      "itself\r\n",
      "develop\r\n",
      "critic\r\n",
      "feel\r\n",
      "more\r\n",
      "crush\r\n",
      "worst\r\n",
      "man\r\n",
      "made\r\n",
      "women\r\n",
      "slow\r\n",
      "silli\r\n",
      "unintent\r\n",
      "hilari\r\n",
      "oddli\r\n",
      "rivet\r\n",
      "documentari\r\n",
      "piano\r\n",
      "teacher\r\n",
      "titl\r\n",
      "charact\r\n",
      "control\r\n",
      "?\r\n",
      "certainli\r\n",
      "decad\r\n",
      "life\r\n",
      "classic\r\n",
      "franchis\r\n",
      "let'\r\n",
      "hope\r\n",
      "problem\r\n",
      "dramat\r\n",
      "premis\r\n",
      "mr\r\n",
      "content\r\n",
      "state\r\n",
      "construct\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 100 features.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_alpha=6.0\n",
    "learn_count=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis(data_x,theta):\n",
    "    '''\n",
    "    仮説関数\n",
    "    data_xからdata_yを予測'''\n",
    "    return 1.0/(1.0+np.exp(-data_x.dot(theta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(data_x,theta,data_y):\n",
    "    '''\n",
    "    目的関数\n",
    "    data_yに対して予測した結果と正解の差を算出'''\n",
    "    m = data_y.size\n",
    "    h = hypothesis(data_x,theta)\n",
    "    j = 1/ m*np.sum(-data_y*np.log(h) - (np.ones(m)-data_y) * np.log(np.ones(m)-h))\n",
    "    \n",
    "    return j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(data_x,theta,data_y):\n",
    "    '''\n",
    "    最急降下法における勾配の算出\n",
    "    '''\n",
    "    \n",
    "    m = data_y.size\n",
    "    h= hypothesis(data_x,theta)\n",
    "    grad = 1/m*(h-data_y).dot(data_x)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data,dict_features):\n",
    "    '''\n",
    "    文章から素性を抽出\n",
    "    '''\n",
    "    data_one_x = np.zeros(len(dict_features)+1,dtype=np.float64)\n",
    "    data_one_x[0]=1\n",
    "    \n",
    "    for word in data.split(' '):\n",
    "        word = word.strip()\n",
    "        if is_stopword(word):\n",
    "            continue\n",
    "        word = ps.stem(word)\n",
    "        \n",
    "        try:\n",
    "            data_one_x[dict_features[word]]=1\n",
    "        except:\n",
    "            pass\n",
    "    return data_one_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict_features():\n",
    "    '''\n",
    "    素性をインデックスに変換する辞書\n",
    "    '''\n",
    "    with codecs.open(fname_features,'r',fencoding) as file_in:\n",
    "        return {line.strip(): i for i,line in enumerate(file_in,start=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_set(sentiments,dict_features):\n",
    "    '''\n",
    "    学習対象の行列と、極性ラベルの行列をreturn\n",
    "    '''\n",
    "    data_x = np.zeros([len(sentiments),len(dict_features)+1],dtype=np.float64)\n",
    "    data_y = np.zeros(len(sentiments),dtype=np.float64)\n",
    "    \n",
    "    for i, line in enumerate(sentiments):\n",
    "        data_x[i] = extract_features(line[3:],dict_features)\n",
    "        if line[0:2]=='+1':\n",
    "            data_y[i]=1\n",
    "    return data_x,data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(data_x,data_y,alpha,count):\n",
    "    '''\n",
    "    logistic reg の学習\n",
    "    '''\n",
    "    theta = np.zeros(data_x.shape[1])\n",
    "    c = cost(data_x,theta,data_y)\n",
    "    print('\\t学習かいし\\t cost:{}'.format(c))\n",
    "    \n",
    "    for i in range(1,count+1):\n",
    "        grad = gradient(data_x,theta,data_y)\n",
    "        theta -= alpha*grad\n",
    "        \n",
    "        if i %100==0:\n",
    "            c = cost(data_x,theta,data_y)\n",
    "            e = np.max(np.absolute(alpha * grad))\n",
    "            \n",
    "            print('\\t学習中(#{})\\tcost:{}\\tE:{}'.format(i,c,e))\n",
    "    c = cost(data_x,theta,data_y)\n",
    "    e = np.max(np.absolute(alpha*grad))\n",
    "    print(\"\\t学習完了(#{})\\t cost:{}\\tE:{}\".format(i,c,e))\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_features = load_dict_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習率:6.0\t学習繰り返し数:1000\n",
      "\t学習かいし\t cost:0.6931471805599453\n",
      "\t学習中(#100)\tcost:0.4867235807923767\tE:0.006382759714404753\n",
      "\t学習中(#200)\tcost:0.4378801012698738\tE:0.0035625972729612055\n",
      "\t学習中(#300)\tcost:0.4102695665075672\tE:0.0025672312512357827\n",
      "\t学習中(#400)\tcost:0.39138225593804876\tE:0.0021004322327511297\n",
      "\t学習中(#500)\tcost:0.3772327785443585\tE:0.0017741643510842176\n",
      "\t学習中(#600)\tcost:0.36604076797985047\tE:0.0015494001749209014\n",
      "\t学習中(#700)\tcost:0.35685875708950915\tE:0.001393726759405461\n",
      "\t学習中(#800)\tcost:0.34912420652102616\tE:0.0012703810950406992\n",
      "\t学習中(#900)\tcost:0.34247693043489486\tE:0.001169977113623327\n",
      "\t学習中(#1000)\tcost:0.3366731553338597\tE:0.0010864905858590868\n",
      "\t学習完了(#1000)\t cost:0.3366731553338597\tE:0.0010864905858590868\n"
     ]
    }
   ],
   "source": [
    "with codecs.open(fname_sentiment,'r',fencoding) as file_in:\n",
    "    data_x,data_y = create_training_set(list(file_in),dict_features)\n",
    "    \n",
    "print(\"学習率:{}\\t学習繰り返し数:{}\".format(learn_alpha,learn_count))\n",
    "theta = learn(data_x,data_y,alpha=learn_alpha,count=learn_count)\n",
    "\n",
    "np.save(fname_theta,theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_features = load_dict_features()\n",
    "theta = np.load(fname_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_one_x = extract_features(review,dict_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h-> 0.4234288486372619\n",
      "label:-1\n"
     ]
    }
   ],
   "source": [
    "h = hypothesis(data_one_x,theta)\n",
    "print(\"h->\",h)\n",
    "if h>0.5:\n",
    "    print(\"label+1\")\n",
    "else:\n",
    "    print(\"label:-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(fname_features,'r',fencoding) as file_in:\n",
    "    feature = list(file_in)\n",
    "theta = np.load(fname_theta)\n",
    "index_sorted = np.argsort(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10\n",
      "\t2.3233484565362064\tengross\n",
      "\t2.30648948096215\trefresh\n",
      "\t2.0010357552489624\tunexpect\n",
      "\t1.8598244770606749\tremark\n",
      "\t1.7798733743017685\texamin\n",
      "\t1.7099157134404346\tresist\n",
      "\t1.6947156223762303\tcaptur\n",
      "\t1.6712997670895995\tdelight\n",
      "\t1.6536397975294048\tconfid\n",
      "\t1.6223701025900543\trefreshingli\n"
     ]
    }
   ],
   "source": [
    "print('top 10')\n",
    "for index in index_sorted[:-11:-1]:\n",
    "    print('\\t{}\\t{}'.format(theta[index],features[index-1].strip() if index >0 else '(none)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worts 10\n",
      "\t-2.6758048728517405\tbore\n",
      "\t-2.4008820178245975\tdull\n",
      "\t-2.2153564633347123\twast\n",
      "\t-2.103540416702802\tfail\n",
      "\t-2.0395296543425876\tbadli\n",
      "\t-2.0001748002734248\tworst\n",
      "\t-1.9471133426132452\tflat\n",
      "\t-1.9437641860859185\tplod\n",
      "\t-1.9361962415934924\tmediocr\n",
      "\t-1.9274423570723673\troutin\n"
     ]
    }
   ],
   "source": [
    "print('worts 10')\n",
    "for index in index_sorted[:10]:\n",
    "    print('\\t{}\\t{}'.format(theta[index],features[index-1].strip() if index >0 else '(none)'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_result=\"result.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_features=load_dict_features()\n",
    "theta = np.load(fname_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(fname_sentiment,'r',fencoding) as file_in:\n",
    "    with open(fname_result,'w' ) as file_out:\n",
    "        for line in file_in:\n",
    "            data_one_x = extract_features(line[3:],dict_features)\n",
    "            \n",
    "            h = hypothesis(data_one_x,theta)\n",
    "            if h>0.5:\n",
    "                file_out.write(\"{}\\t{}\\t{}\\n\".format(line[0:2],'+1',h))\n",
    "            else:\n",
    "                file_out.write(\"{}\\t{}\\t{}\\n\".format(line[0:2],'-1',h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\t-1\t0.02184241302738834\r\n",
      "+1\t+1\t0.8399349925940273\r\n",
      "+1\t+1\t0.9524430656464458\r\n",
      "+1\t+1\t0.5211158153710528\r\n",
      "+1\t+1\t0.7148875324972783\r\n",
      "-1\t+1\t0.7742974466112117\r\n",
      "-1\t-1\t0.18509774706230664\r\n",
      "-1\t-1\t0.04202627002553687\r\n",
      "-1\t-1\t0.2342336275938971\r\n",
      "+1\t+1\t0.9085934249665856\r\n"
     ]
    }
   ],
   "source": [
    "! head result.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(fname):\n",
    "    TP = 0      # True-Positive     予想が+1、正解も+1\n",
    "    FP = 0      # False-Positive    予想が+1、正解は-1\n",
    "    FN = 0      # False-Negative    予想が-1、正解は+1\n",
    "    TN = 0      # True-Negative     予想が-1、正解も-1\n",
    "    with open(fname) as data_file:\n",
    "        for line in data_file:\n",
    "            cols = line.split('\\t')\n",
    "            \n",
    "            if len(cols)<3:\n",
    "                continue\n",
    "            if cols[0]=='+1':\n",
    "                if cols[1]=='+1':\n",
    "                    TP+=1\n",
    "                else:\n",
    "                    FN+=1\n",
    "            else:\n",
    "                if cols[1]==\"+1\":\n",
    "                    FP+=1\n",
    "                else:\n",
    "                    TN +=1\n",
    "                    \n",
    "    accuracy = (TP+TN) / (TP+FP+FN+TN)#正解率\n",
    "    precision = TP/(TP+FP)#適合りつ\n",
    "    recall = TP/(TP+FN)#再現率\n",
    "    f1 = (2*recall*precision) /(recall+precision)#F1\n",
    "    return accuracy,precision,recall,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正解率　\t0.8643781654473832\n",
      "適合率　\t0.8657503295048014\n",
      "再現率　\t0.8625023447758394\n",
      "F1スコア　\t0.8641232850967863\n"
     ]
    }
   ],
   "source": [
    "print('正解率　\\t{}\\n適合率　\\t{}\\n再現率　\\t{}\\nF1スコア　\\t{}'.format(*score(fname_result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_features = load_dict_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(fname_sentiment,'r',fencoding) as file_in:\n",
    "    sentiments_all = list(file_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "division=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = []\n",
    "unit = int(len(sentiments_all) / division)\n",
    "for i in range(5):\n",
    "    sentiments.append(sentiments_all[i+unit:(i+1)*unit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5\n",
      "\t学習かいし\t cost:0.6931471805599453\n",
      "\t学習中(#100)\tcost:0.47300235702049015\tE:0.00635449496408459\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-a36ea2734c99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mdata_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_training_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_learn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdict_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearn_alpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearn_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-fea580591b3a>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(data_x, data_y, alpha, count)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mtheta\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-99c5404cacd9>\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(data_x, theta, data_y)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mh\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mhypothesis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdata_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open(fname_result,'w' ) as file_out:\n",
    "    for i in range(division):\n",
    "        print(\"{}/{}\".format(i+1,division))\n",
    "        \n",
    "        data_learn=[]\n",
    "        for j in range(division):\n",
    "            if i==j:\n",
    "                data_validation = sentiments[j]\n",
    "            else:\n",
    "                data_learn += sentiments[j]\n",
    "        \n",
    "        data_x,data_y = create_training_set(data_learn,dict_features)\n",
    "        theta = learn(data_x,data_y,alpha=learn_alpha,count=learn_count)\n",
    "        \n",
    "        for line in data_validation:\n",
    "            data_one_x = extract_features(line[3:],dict_features)\n",
    "            \n",
    "            h = hypothesis(data_one_x,theta)\n",
    "            if h > 0.5:\n",
    "                file_out.write('{}\\t{}\\t{}\\n'.format(line[0:2], '+1', h))\n",
    "            else:\n",
    "                file_out.write('{}\\t{}\\t{}\\n'.format(line[0:2], '-1', 1 - h))\n",
    "print('正解率　\\t{}\\n適合率　\\t{}\\n再現率　\\t{}\\nF1スコア　\\t{}'.format(*score(fname_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
